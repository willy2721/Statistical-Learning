---
title: "otto"
output: html_notebook
---

```{r}
library(ggplot2)
library(randomForest)
library(readr)
library(dplyr)
library(caret)
library(adabag)
library(rminer)
library(class)
library(kknn)
library(xgboost)
library(methods)
library(data.table)
library(magrittr)
options( java.parameters = "-Xmx2g" )
library(extraTrees)
library(CORElearn)

source("model_ada.R")
source("model_xgboost.R")
source("rf_prob_calibration.R")
#source("rf_on_calibrate.R")
source("utility.R")
```

```{r}
train <- read.csv("C:/Users/Willy/OneDrive/公用/台大/Senior courses/Second semester/Statistical Learning/Otto/data/train.csv", na.strings = "")
test <- read.csv("C:/Users/Willy/OneDrive/公用/台大/Senior courses/Second semester/Statistical Learning/Otto/data/test.csv", na.strings = "")
```

```{r}
# Get answer for train
train_answer <- myanswer(train)
#head(train_answer)
```



```{r}
# Simple Random Forest implementation
rf_submission <- data.frame(id=test$id, Class_1=NA, Class_2=NA, Class_3=NA, Class_4=NA, Class_5=NA, Class_6=NA, Class_7=NA, Class_8=NA, Class_9=NA)
rf <- randomForest(train[,c(-1,-95)], as.factor(train$target), ntree=800, mtry = 20, nodesize = 1)
rf_submission[,2:10] <- predict(rf, test[,c(-1,-95)], type="prob")
write.csv(rf_submission, file="random_forest_submission.csv", row.names = FALSE)

```


```{r}
# Try train / calibrate percentages of 0.7 (0.49 log loss !)
rf_cali_70 <- rf_on_calibrate(train, 0.7, 800)
class_cali_70 <- rf_prob_calibration(rf_cali_70[['subtrain']], rf_cali_70[['caliset']], rf_cali_70[['cali_answer']], rf_cali_70[['subtrain_rf']], 0.7, 800)

# Predict for test set 
test_predict_70 <- predict(rf_cali_70[['subtrain_rf']], test[,-1], type="prob")

for(i in 1:9){
    # Calibrate the probabilities with pre-stored calibration
    test_predict_70[,i] <- applyCalibration(test_predict_70[,i], class_cali_70[["cali_model"]][[i]])
}

rf_calibrated_submission_70 <- data.frame(id=test$id, Class_1=NA, Class_2=NA, Class_3=NA, Class_4=NA, Class_5=NA, Class_6=NA, Class_7=NA, Class_8=NA, Class_9=NA)
# This can be used as the meta feature for test !
rf_calibrated_submission_70[,2:10] <- test_predict_70
write.csv(rf_calibrated_submission_70, file="rf_calibrated_submission_70.csv", row.names = FALSE)
```



```{r}
cali_cv_start_time <- Sys.time()

# Cross-validate calibration
folds <- createFolds(train$target, k = 3, list = FALSE)
train$fold <- folds

# Create empty dataframe to store meta feature for train
train_meta <- data.frame()
test_predict_combined <- matrix(0, nrow = 144368, ncol = 9)

# K-fold cross-validation
for(k in 1:3){
  
  # Seperate the training, calibration folds    
  calibrate_i <- which(train$fold == k)
  subtrain <- train[-calibrate_i, ]
  caliset <- train[calibrate_i, ]
  cali_answer <- train_answer[calibrate_i,]
  
  # Remember to remove folds when training !
  # Train random forest on subtrain
  subtrain_rf <- randomForest(subtrain[,c(-1,-95,-96)], as.factor(subtrain$target), ntree = 800, mtry = 20, nodesize = 1)
  # Predict for calibrate set 
  cali_predict <- predict(subtrain_rf, caliset[,c(-1,-95,-96)], type="prob")
  # Predict for test set 
  test_predict <- predict(subtrain_rf, test[,-1], type="prob")
  
  # Loop through each class
  for(i in 1:9){
    # Store predicted class probability and correct class
    pred_is_class <- cali_predict[,i]
    ans_is_class <- factor(cali_answer[,i + 1])
    ans_is_class <- factor(ans_is_class,levels(ans_is_class)[c(2,1)])
    
    # Calibrate the probabilities for test set
    calibration <- calibrate(ans_is_class, pred_is_class, class1 = 1, method = "isoReg", weight=NULL, noBins=10, assumeProbabilities=TRUE)
    test_predict[,i] <- applyCalibration(test_predict[,i], calibration)
    cali_predict[,i] <- applyCalibration(cali_predict[,i], calibration)
  } 
  
  # Add prediction to test_predict_combined and train_meta
  test_predict_combined <- test_predict_combined + test_predict
  rbind(train_meta, cali_predict)
  
}

# Get the average of three test_predictions (also meta feature for test)
test_predict_combined <- test_predict_combined / 3

# Generate submission
rf_calibrated_submission_combined <- data.frame(id=test$id, Class_1=NA, Class_2=NA, Class_3=NA, Class_4=NA, Class_5=NA, Class_6=NA, Class_7=NA, Class_8=NA, Class_9=NA)
rf_calibrated_submission_combined[,2:10] <- test_predict_combined
write.csv(rf_calibrated_submission_combined, file="rf_calibrated_submission_combined", row.names = FALSE)


# Order meta feature dataframe by row name
 train_meta[order(as.numeric(row.names(train_meta))),]
 
cali_cv_end_time <- Sys.time()
cali_cv_exec_time <- cali_cv_end_time - cali_cv_start_time


```




```{r}
############################################################
# Compare logloss and reliability with different calibration methods
# Create matrix for prediction probabilities after isoReg and Platt
prob_iso_80 <- c()
prob_platt_80 <- c()
for(i in 1:9){
  prob_iso_80 <- cbind(prob_iso_80, class_cali_80[["class_probs"]][[i]]$IsoReg_Calibrated)
  prob_platt_80 <- cbind(prob_platt_80, class_cali_80[["class_probs"]][[i]]$Platt_Calibrated)
}

# Evaluate logloss for original predicted probabilities and calibrated probabilities
cali_80_ans <- rf_cali[['cali_answer']][,-1]
logloss_80 <- c(logloss(class_cali_80$cali_predict,cali_80_ans),logloss(prob_iso_80,cali_80_ans), logloss(prob_platt_80,cali_80_ans))
names(logloss_80) <- c("Original","Iso-calibrated", "Platt-calibrated")
logloss_80

# Probability of class 1 before and after calibration
class_cali_80[['class_probs']][[1]]$Probability
class_cali_80[['class_probs']][[1]]$IsoReg_Calibrated
class_cali_80[['class_probs']][[1]]$Platt_Calibrated

# Correct answer for class 1
cali_80_ans[,1]

# Logloss for each class
logloss(class_cali_80[['class_probs']][[1]]$IsoReg_Calibrated, cali_80_ans[,1])
logloss(class_cali_80[['class_probs']][[1]]$Platt_Calibrated, cali_80_ans[,1])


ggplot(calibration(Class ~ Probability + IsoReg_Calibrated + Platt_Calibrated, data = class_cali_80[['class_probs']][[3]], class = "1")) + geom_line() + ggtitle("Reliability plot of different calibration methods")
############################################################


```



```{r}
############################################################

### Visualization ###
# Plot a histogram
ans_class_1_hist <- ggplot(ans_class_1_prob, aes(x = Probability)) + geom_histogram(binwidth = 0.02) + facet_grid(Class ~ .) + xlab("Probability Segment")
ans_class_1_hist

# Plot calibration
# Method 1 using caret and ggplot2
plot(calibration(Class ~ Probability + IsoReg_Calibrated + Platt_Calibrated, data = ans_class_1_prob, class = "1"))


# Method 2 using CORElearn package
reliabilityPlot(pred_is_class_1, ans_is_class_1, titleText="Class 1 reliability", boxing="equidistant", noBins=10, classValue = 1, printWeight=FALSE)

############################################################
```






```{r}
# Simple extraTrees implementation # NOT ENOUGH MEMORY!!
#extraTrees_submission <- data.frame(id=test$id, Class_1=NA, Class_2=NA, Class_3=NA, Class_4=NA, Class_5=NA, Class_6=NA, Class_7=NA, Class_8=NA, Class_9=NA)
#extra <- extraTrees(train[,c(-1,-95)], as.factor(train$target), mtry = 20, ntree=600, nodesize = 1, numThreads = 2)
#extraTrees_submission[,2:10] <- predict(extra, test[,c(-1,-95)], probability = TRUE)
#write.csv(extraTrees_submission, file="extraTrees_submission.csv", row.names = FALSE)
#

```

```{r}
# Simple knn classification (k = 128, distance = 2)
kknn_128_submission <- data.frame(id=test$id, Class_1=NA, Class_2=NA, Class_3=NA, Class_4=NA, Class_5=NA, Class_6=NA, Class_7=NA, Class_8=NA, Class_9=NA)
kknn.128 <- kknn(target ~ .-id, train, test, na.action = na.omit(), k = 128, distance = 2, kernel = "optimal")
kknn_128_submission[,2:10] <- kknn.128$prob
write.csv(kknn_128_submission, file="kknn_128_submission.csv", row.names = FALSE)

```




```{r}
# Code for cross-validation
# TAKES MAJOR TIME ... IF I GET RICH AND BUY A WORKSTATION ...
# Creating folds with caret
folds <- createFolds(train$target, k = 10, list = FALSE)
train$fold <- folds

# Create vector to store log-loss for each fold and execution time
fold_log_loss <- c()
fold_rf_exec_time <- c()

# K-fold cross-validation
for(k in 1:10){
  
  # Measure execution time
  start_time <- Sys.time()
  
  # Seperate the training, testing folds    
  test_i <- which(train$fold == k)
  train_fold <- train[-test_i, ]
  test_fold <- train[test_i, ]
  ans_fold <- answer[test_i,]
  train_fold$fold <- NULL
  test_fold$fold <- NULL
  
  ### THIS IS THE PART WHERE YOU TRY DIFFERENT MODELS
  
  # Perform simple random forest on trainfold
  prediction <- data.frame(id=test_fold$id, Class_1=NA, Class_2=NA, Class_3=NA, Class_4=NA, Class_5=NA, Class_6=NA, Class_7=NA, Class_8=NA, Class_9=NA)
  rf_train <- randomForest(train_fold[,c(-1,-95)], as.factor(train_fold$target), ntree=200, importance=TRUE)
  
  # Test simple random forest on testfold
  prediction[,2:10] <- predict(rf_train, test_fold[,-1], type="prob")
  
  # Evaluate log-loss
  fold_log_loss[k] <- logloss(as.matrix(prediction[,-1]),as.matrix(ans_fold[,-1]))
  
  end_time <- Sys.time()
  fold_rf_exec_time[k] <- end_time - start_time
}


fold_log_loss
mean(fold_log_loss)

```



