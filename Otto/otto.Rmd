---
title: "otto"
output: html_notebook
---

```{r}
library(ggplot2)
library(randomForest)
library(readr)
library(dplyr)
library(caret)
library(adabag)
library(rminer)
library(class)
library(kknn)
library(xgboost)
library(methods)
library(data.table)
library(magrittr)
#options( java.parameters = "-Xmx2g" )
#library(extraTrees)
library(CORElearn)

source("model_ada.R")
source("model_xgboost.R")
source("rf_prob_calibration.R")
#source("rf_on_calibrate.R")
source("utility.R")
```

```{r}
train <- read.csv("C:/Users/Willy/OneDrive/公用/台大/Senior courses/Second semester/Statistical Learning/Otto/data/train.csv", na.strings = "")
test <- read.csv("C:/Users/Willy/OneDrive/公用/台大/Senior courses/Second semester/Statistical Learning/Otto/data/test.csv", na.strings = "")
```

```{r}
# Get answer for train
train_answer <- myanswer(train)
#head(train_answer)
```



```{r}
# Simple Random Forest implementation
rf_submission <- data.frame(id=test$id, Class_1=NA, Class_2=NA, Class_3=NA, Class_4=NA, Class_5=NA, Class_6=NA, Class_7=NA, Class_8=NA, Class_9=NA)
rf <- randomForest(train[,c(-1,-95)], as.factor(train$target), ntree=800, mtry = 20, nodesize = 1)
rf_submission[,2:10] <- predict(rf, test[,c(-1,-95)], type="prob")
write.csv(rf_submission, file="random_forest_submission.csv", row.names = FALSE)

```


```{r}
# Try train / calibrate percentages of 0.7 (0.49 log loss)
rf_cali_70 <- rf_on_calibrate(train, 0.7, 800)
class_cali_70 <- rf_prob_calibration(rf_cali_70[['subtrain']], rf_cali_70[['caliset']], rf_cali_70[['cali_answer']], rf_cali_70[['subtrain_rf']], 0.7, 800)

# Predict for test set 
test_predict_70 <- predict(rf_cali_70[['subtrain_rf']], test[,-1], type="prob")

for(i in 1:9){
    # Calibrate the probabilities with pre-stored calibration
    test_predict_70[,i] <- applyCalibration(test_predict_70[,i], class_cali_70[["cali_model"]][[i]])
}

rf_calibrated_submission_70 <- data.frame(id=test$id, Class_1=NA, Class_2=NA, Class_3=NA, Class_4=NA, Class_5=NA, Class_6=NA, Class_7=NA, Class_8=NA, Class_9=NA)
# This can be used as the meta feature for test !
rf_calibrated_submission_70[,2:10] <- test_predict_70
write.csv(rf_calibrated_submission_70, file="rf_calibrated_submission_70.csv", row.names = FALSE)

###############################################################



```

```{r}
# Try some ensembling techniques

# Meta features for random forest
for(rounds in 1 : 10){
  rf_cali_bag_list <- rf_cali_cv_bagged(train, test, 1, 3)
  save(rf_cali_bag_list, file = paste(paste("rf_cali_", toString(rounds), sep = ""), ".rData", sep = ""))
  rm(rf_cali_bag_list)
}

rf_bag_list <- list()
for(rounds in 1 : 10){
  load(paste(paste("rf_cali_", toString(rounds), sep = ""), ".rData", sep = ""))
  rf_bag_list[[rounds]] <- rf_cali_bag_list
  if(rounds == 1){
    train_meta_rf <- rf_bag_list[[1]][[1]]
    test_meta_rf <- rf_bag_list[[1]][[2]]
  }
  else{
    train_meta_rf <- train_meta_rf + rf_bag_list[[rounds]][[1]]
    test_meta_rf <- test_meta_rf + rf_bag_list[[rounds]][[2]]
  }
}
# Scale matrix
train_meta_rf <- as.matrix(train_meta_rf) / rowSums(as.matrix(train_meta_rf))
test_meta_rf <- test_meta_rf / rowSums(test_meta_rf)

# Change to dataframe and add row, column names
train_meta_rf <- as.data.frame(train_meta_rf)
test_meta_rf <- as.data.frame(test_meta_rf)
rfcolname <- c("rf1", "rf2", "rf3", "rf4", "rf5", "rf6", "rf7", "rf8", "rf9")
colnames(train_meta_rf) <- rfcolname
colnames(test_meta_rf) <- rfcolname
rownames(train_meta_rf) <- c(1:nrow(train_meta_rf))
rownames(test_meta_rf) <- c(1:nrow(test_meta_rf))


# Meta features for xgboost
#xgb_bag_list <- xgb_bagged(train, test, 10, 3)
xgbcolname <- c("xgb1", "xgb2", "xgb3", "xgb4", "xgb5", "xgb6", "xgb7", "xgb8", "xgb9")
# Scale matrix
#xgb_bag_list[['train_meta_bagged']] <- xgb_bag_list[['train_meta_bagged']] / rowSums(xgb_bag_list[['train_meta_bagged']])
#xgb_bag_list[['test_meta_bagged']] <- xgb_bag_list[['test_meta_bagged']] / rowSums(xgb_bag_list[['test_meta_bagged']])
train_meta_xgb <- as.data.frame(xgb_bag_list[['train_meta_bagged']])
test_meta_xgb <- as.data.frame(xgb_bag_list[['test_meta_bagged']])
colnames(train_meta_xgb) <- xgbcolname
colnames(test_meta_xgb) <- xgbcolname
rownames(train_meta_xgb) <- c(1:nrow(train_meta_xgb))
rownames(test_meta_xgb) <- c(1:nrow(test_meta_xgb))

# Meta features for knn
knncolname <- c("knn1", "knn2", "knn3", "knn4", "knn5", "knn6", "knn7", "knn8", "knn9")
train_meta_knn <- as.data.frame(train_meta_knn_128)
test_meta_knn <- as.data.frame(test_meta_knn_128[2:10])
colnames(train_meta_knn) <- knncolname
colnames(test_meta_knn) <- knncolname
rownames(train_meta_knn) <- c(1:nrow(train_meta_knn))
rownames(test_meta_knn) <- c(1:nrow(test_meta_knn))


# Combine as metatrain
metatrain <- cbind(train[,1:94], train_meta_xgb)
metatrain <- cbind(metatrain, train_meta_rf)
metatrain <- cbind(metatrain, train_meta_knn)
target <- train$target
metatrain <- cbind(metatrain, target)

# Combine as metatest
metatest <- cbind(test, test_meta_xgb)
metatest <- cbind(metatest, test_meta_rf)
metatest <- cbind(metatest, test_meta_knn)



meta <- list(metatrain = metatrain, metatest = metatest)
save(meta, file="meta.RData")
#metatrain$target <- NULL

# Check if normalized
#sum(metatrain[1,95:103])
#sum(metatest[1,95:103])
#sum(metatrain[1,104:112])
#sum(metatest[1,104:112])
#sum(metatrain[1,113:121])
#sum(metatest[1,113:121])

```

```{r}
# Ensemble

#TUNING THE TOP MODEL
#1. Tune nrounds
metatrain[,2:94] <- NULL
metatest[,2:94] <- NULL
metalist <- toxgbmatrix(metatrain, metatest)
param <- list(booster = "gbtree", objective = "multi:softprob", eval_metric = "mlogloss",  eta = 0.1, gamma = 0, max_depth = 5, min_child_weight = 3, subsample=0.8, colsample_bytree=0.8, num_class = 9)
xgb_meta_cv <- xgb.cv( params = param, data = metalist[['train_matrix']], nrounds = 10000, nfold = 3, label = metalist[['num_target']], prediction = FALSE, showsd = T, stratified = T, early_stopping_rounds = 20, maximize = F, print_every_n = 5)
nrounds1 <- xgb_meta_cv$best_iteration

#2.1 Tune max_depth and min_child_weight
# XGBoost
xgb_trcontrol_1 = trainControl(method = "cv", number = 3, verboseIter = TRUE, returnData = TRUE, returnResamp = "all", classProbs = TRUE, summaryFunction = mnLogLoss,
                                 allowParallel = TRUE)
# Best at max_depth = 5, min_child_weight = 1.17
xgbgrid <- expand.grid(nrounds=nrounds1, eta = 0.1, gamma = 0, max_depth = 5, min_child_weight = c(1.15,1.17,1.19), subsample=0.8, colsample_bytree=0.8)

#2.1 Tune gamma (best at gamma = 0.83)
xgbgrid <- expand.grid(nrounds=nrounds1, eta = 0.1, gamma = 0.83, max_depth = 5, min_child_weight = 1.17, subsample=0.8, colsample_bytree=0.8)

# Recalibrate number of boosting rounds
param <- list(booster = "gbtree", objective = "multi:softprob", eval_metric = "mlogloss",  eta = 0.01, gamma = 0.83, max_depth = 5, min_child_weight = 1.17, subsample=0.9, colsample_bytree=0.86, num_class = 9)
xgb_meta_cv2 <- xgb.cv( params = param, data = metalist[['train_matrix']], nrounds = 10000, nfold = 3, label = metalist[['num_target']], prediction = FALSE, showsd = T, stratified = T, early_stopping_rounds = 20, maximize = F, print_every_n = 5)
nrounds2 <- xgb_meta_cv2$best_iteration


#2.1 Tune subsample and column sample by tree (subsample = 0.9, colsample_bytree = 0.86)
xgbgrid <- expand.grid(nrounds=nrounds2, eta = 0.1, gamma = 0.83, max_depth = 5, min_child_weight = 1.17, subsample=0.9, colsample_bytree=0.86)

tuning_top <- train(x = metalist[['train_matrix']], y = as.factor(metalist[['target']]), trControl = xgb_trcontrol_1, tuneGrid = xgbgrid, method = "xgbTree")
save(tuning_top, file="tuning_top.RData")

#3 Tune nrounds for smaller eta one last time
xgb_meta_cv3 <- xgb.cv( params = param, data = metalist[['train_matrix']], nrounds = 10000, nfold = 3, label = metalist[['num_target']], prediction = FALSE, showsd = T, stratified = T, early_stopping_rounds = 20, maximize = F, print_every_n = 5)
# Best parameters : nrounds = 1417, eta = 0.01, gamma = 0.83, max_depth = 5, min_child_weight = 1.17, subsample = 0.9, colsample_bytree = 0.86

#4 Train on full data set
xgboost_final_submission <- data.frame(id=test$id, Class_1=NA, Class_2=NA, Class_3=NA, Class_4=NA, Class_5=NA, Class_6=NA, Class_7=NA, Class_8=NA, Class_9=NA)
param <- list(booster = "gbtree", objective = "multi:softprob", eval_metric = "mlogloss",  eta = 0.01, gamma = 0.83, max_depth = 5, min_child_weight = 1.17, subsample = 0.9, colsample_bytree = 0.86, num_class = 9)
xgb_final <- xgboost(param = param, data = metalist[['train_matrix']], label = metalist[['num_target']], nrounds = 1417)
xgboost_final_submission[,2:10] <- matrix(predict(xgb_final, metalist[['test_matrix']]), ncol = 9, byrow = T)
xgb_final[['evaluation_log']]$train_mlogloss[1417]

write.csv(xgboost_final_submission, file="xgboost_final_submission.csv", row.names = FALSE)

xgboost_final_submission
```

```{r}
metalist[['train_matrix']]
metalist[['test_matrix']]
```





```{r}
############################################################
# Compare logloss and reliability with different calibration methods
# Create matrix for prediction probabilities after isoReg and Platt
prob_iso_80 <- c()
prob_platt_80 <- c()
for(i in 1:9){
  prob_iso_80 <- cbind(prob_iso_80, class_cali_80[["class_probs"]][[i]]$IsoReg_Calibrated)
  prob_platt_80 <- cbind(prob_platt_80, class_cali_80[["class_probs"]][[i]]$Platt_Calibrated)
}

# Evaluate logloss for original predicted probabilities and calibrated probabilities
cali_80_ans <- rf_cali[['cali_answer']][,-1]
logloss_80 <- c(logloss(class_cali_80$cali_predict,cali_80_ans),logloss(prob_iso_80,cali_80_ans), logloss(prob_platt_80,cali_80_ans))
names(logloss_80) <- c("Original","Iso-calibrated", "Platt-calibrated")
logloss_80

# Probability of class 1 before and after calibration
class_cali_80[['class_probs']][[1]]$Probability
class_cali_80[['class_probs']][[1]]$IsoReg_Calibrated
class_cali_80[['class_probs']][[1]]$Platt_Calibrated

# Correct answer for class 1
cali_80_ans[,1]

# Logloss for each class
logloss(class_cali_80[['class_probs']][[1]]$IsoReg_Calibrated, cali_80_ans[,1])
logloss(class_cali_80[['class_probs']][[1]]$Platt_Calibrated, cali_80_ans[,1])


ggplot(calibration(Class ~ Probability + IsoReg_Calibrated + Platt_Calibrated, data = class_cali_80[['class_probs']][[3]], class = "1")) + geom_line() + ggtitle("Reliability plot of different calibration methods")
############################################################


```



```{r}
############################################################

### Visualization ###
# Plot a histogram
ans_class_1_hist <- ggplot(ans_class_1_prob, aes(x = Probability)) + geom_histogram(binwidth = 0.02) + facet_grid(Class ~ .) + xlab("Probability Segment")
ans_class_1_hist

# Plot calibration
# Method 1 using caret and ggplot2
plot(calibration(Class ~ Probability + IsoReg_Calibrated + Platt_Calibrated, data = ans_class_1_prob, class = "1"))


# Method 2 using CORElearn package
reliabilityPlot(pred_is_class_1, ans_is_class_1, titleText="Class 1 reliability", boxing="equidistant", noBins=10, classValue = 1, printWeight=FALSE)

############################################################
```






```{r}
# Simple extraTrees implementation # NOT ENOUGH MEMORY!!
#extraTrees_submission <- data.frame(id=test$id, Class_1=NA, Class_2=NA, Class_3=NA, Class_4=NA, Class_5=NA, Class_6=NA, Class_7=NA, Class_8=NA, Class_9=NA)
#extra <- extraTrees(train[,c(-1,-95)], as.factor(train$target), mtry = 20, ntree=600, nodesize = 1, numThreads = 2)
#extraTrees_submission[,2:10] <- predict(extra, test[,c(-1,-95)], probability = TRUE)
#write.csv(extraTrees_submission, file="extraTrees_submission.csv", row.names = FALSE)
#

```

```{r}
# Simple knn classification (k = 128, distance = 2)
kknn_128_submission <- data.frame(id=test$id, Class_1=NA, Class_2=NA, Class_3=NA, Class_4=NA, Class_5=NA, Class_6=NA, Class_7=NA, Class_8=NA, Class_9=NA)
kknn.128 <- kknn(target ~ .-id, train, test, na.action = na.omit(), k = 128, distance = 2, kernel = "optimal")
kknn_128_submission[,2:10] <- kknn.128$prob
write.csv(kknn_128_submission, file="kknn_128_submission.csv", row.names = FALSE)

test_meta_knn_128 <- read.csv("kknn_128_submission.csv")
kknn_128_train <- kknn(target ~ .-id, train, train[,-95], na.action = na.omit(), k = 128, distance = 2, kernel = "optimal")
train_meta_knn_128 <- kknn_128_train$prob

```




```{r}
# Code for cross-validation
# TAKES MAJOR TIME ... IF I GET RICH AND BUY A WORKSTATION ...
# Creating folds with caret
#folds <- createFolds(train$target, k = 10, list = FALSE)
#train$fold <- folds

# Create vector to store log-loss for each fold and execution time
#fold_log_loss <- c()
#fold_rf_exec_time <- c()

# K-fold cross-validation
#for(k in 1:10){
  
  # Measure execution time
#  start_time <- Sys.time()
  
  # Seperate the training, testing folds    
#  test_i <- which(train$fold == k)
#  train_fold <- train[-test_i, ]
#  test_fold <- train[test_i, ]
#  ans_fold <- answer[test_i,]
#  train_fold$fold <- NULL
#  test_fold$fold <- NULL
  
  ### THIS IS THE PART WHERE YOU TRY DIFFERENT MODELS
  
  # Perform simple random forest on trainfold
#  prediction <- data.frame(id=test_fold$id, Class_1=NA, Class_2=NA, Class_3=NA, Class_4=NA, Class_5=NA, Class_6=NA, Class_7=NA, Class_8=NA, Class_9=NA)
#  rf_train <- randomForest(train_fold[,c(-1,-95)], as.factor(train_fold$target), ntree=200, importance=TRUE)
  
  # Test simple random forest on testfold
#  prediction[,2:10] <- predict(rf_train, test_fold[,-1], type="prob")
  
  # Evaluate log-loss
#  fold_log_loss[k] <- logloss(as.matrix(prediction[,-1]),as.matrix(ans_fold[,-1]))
  
#  end_time <- Sys.time()
#  fold_rf_exec_time[k] <- end_time - start_time
#}


#fold_log_loss
#mean(fold_log_loss)

```



