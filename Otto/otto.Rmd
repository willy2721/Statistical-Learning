---
title: "otto"
output: html_notebook
---

```{r}
library(ggplot2)
library(randomForest)
library(readr)
library(dplyr)
library(caret)
library(adabag)
library(rminer)
library(class)
library(kknn)
library(xgboost)
library(methods)
library(data.table)
library(magrittr)
options( java.parameters = "-Xmx2g" )
library(extraTrees)
library(CORElearn)

source("model_ada.R")
source("model_xgboost.R")
source("rf_prob_calibration.R")
#source("rf_on_calibrate.R")
source("utility.R")
```

```{r}
train <- read.csv("C:/Users/Willy/OneDrive/??????/??????/Senior courses/Second semester/Statistical Learning/Otto/data/train.csv", na.strings = "")
test <- read.csv("C:/Users/Willy/OneDrive/??????/??????/Senior courses/Second semester/Statistical Learning/Otto/data/test.csv", na.strings = "")
```

```{r}
# Get answer for train
train_answer <- myanswer(train)
#head(train_answer)
```



```{r}
# Simple Random Forest implementation
rf_submission <- data.frame(id=test$id, Class_1=NA, Class_2=NA, Class_3=NA, Class_4=NA, Class_5=NA, Class_6=NA, Class_7=NA, Class_8=NA, Class_9=NA)
rf <- randomForest(train[,c(-1,-95)], as.factor(train$target), ntree=800, mtry = 20, nodesize = 1)
rf_submission[,2:10] <- predict(rf, test[,c(-1,-95)], type="prob")
write.csv(rf_submission, file="random_forest_submission.csv", row.names = FALSE)

```


```{r}
# Try train / calibrate percentages of 0.7 (0.49 log loss !)
rf_cali_70 <- rf_on_calibrate(train, 0.7, 800)
class_cali_70 <- rf_prob_calibration(rf_cali_70[['subtrain']], rf_cali_70[['caliset']], rf_cali_70[['cali_answer']], rf_cali_70[['subtrain_rf']], 0.7, 800)

# Predict for test set 
test_predict_70 <- predict(rf_cali_70[['subtrain_rf']], test[,-1], type="prob")

for(i in 1:9){
    # Calibrate the probabilities with pre-stored calibration
    test_predict_70[,i] <- applyCalibration(test_predict_70[,i], class_cali_70[["cali_model"]][[i]])
}

rf_calibrated_submission_70 <- data.frame(id=test$id, Class_1=NA, Class_2=NA, Class_3=NA, Class_4=NA, Class_5=NA, Class_6=NA, Class_7=NA, Class_8=NA, Class_9=NA)
rf_calibrated_submission_70[,2:10] <- test_predict_70
write.csv(rf_calibrated_submission_70, file="rf_calibrated_submission_70.csv", row.names = FALSE)
```


```{r}
# Cross-validate calibration
folds <- createFolds(train$target, k = 3, list = FALSE)
# Remember to remove folds when training !
train$fold <- folds

# K-fold cross-validation
for(k in 1:3){
  # Seperate the training, calibration folds    
  calibrate_i <- which(train$fold == k)
  subtrain <- train[-calibrate_i, ]
  caliset <- train[calibrate_i, ]
  cali_answer <- train_answer[calibrate_i,]
  
  #train_fold$fold <- NULL
  #test_fold$fold <- NULL
}

```




```{r}
############################################################
# Compare logloss and reliability with different calibration methods
# Create matrix for prediction probabilities after isoReg and Platt
prob_iso_80 <- c()
prob_platt_80 <- c()
for(i in 1:9){
  prob_iso_80 <- cbind(prob_iso_80, class_cali_80[["class_probs"]][[i]]$IsoReg_Calibrated)
  prob_platt_80 <- cbind(prob_platt_80, class_cali_80[["class_probs"]][[i]]$Platt_Calibrated)
}

# Evaluate logloss for original predicted probabilities and calibrated probabilities
cali_80_ans <- rf_cali[['cali_answer']][,-1]
logloss_80 <- c(logloss(class_cali_80$cali_predict,cali_80_ans),logloss(prob_iso_80,cali_80_ans), logloss(prob_platt_80,cali_80_ans))
names(logloss_80) <- c("Original","Iso-calibrated", "Platt-calibrated")
logloss_80

# Probability of class 1 before and after calibration
class_cali_80[['class_probs']][[1]]$Probability
class_cali_80[['class_probs']][[1]]$IsoReg_Calibrated
class_cali_80[['class_probs']][[1]]$Platt_Calibrated

# Correct answer for class 1
cali_80_ans[,1]

# Logloss for each class
logloss(class_cali_80[['class_probs']][[1]]$IsoReg_Calibrated, cali_80_ans[,1])
logloss(class_cali_80[['class_probs']][[1]]$Platt_Calibrated, cali_80_ans[,1])


ggplot(calibration(Class ~ Probability + IsoReg_Calibrated + Platt_Calibrated, data = class_cali_80[['class_probs']][[3]], class = "1")) + geom_line() + ggtitle("Reliability plot of different calibration methods")
############################################################
```



```{r}
############################################################

### Visualization ###
# Plot a histogram
ans_class_1_hist <- ggplot(ans_class_1_prob, aes(x = Probability)) + geom_histogram(binwidth = 0.02) + facet_grid(Class ~ .) + xlab("Probability Segment")
ans_class_1_hist

# Plot calibration
# Method 1 using caret and ggplot2
plot(calibration(Class ~ Probability + IsoReg_Calibrated + Platt_Calibrated, data = ans_class_1_prob, class = "1"))


# Method 2 using CORElearn package
reliabilityPlot(pred_is_class_1, ans_is_class_1, titleText="Class 1 reliability", boxing="equidistant", noBins=10, classValue = 1, printWeight=FALSE)

############################################################
```






```{r}
# Simple extraTrees implementation # NOT ENOUGH MEMORY!!
#extraTrees_submission <- data.frame(id=test$id, Class_1=NA, Class_2=NA, Class_3=NA, Class_4=NA, Class_5=NA, Class_6=NA, Class_7=NA, Class_8=NA, Class_9=NA)
#extra <- extraTrees(train[,c(-1,-95)], as.factor(train$target), mtry = 20, ntree=600, nodesize = 1, numThreads = 2)
#extraTrees_submission[,2:10] <- predict(extra, test[,c(-1,-95)], probability = TRUE)
#write.csv(extraTrees_submission, file="extraTrees_submission.csv", row.names = FALSE)
#

```

```{r}
# Simple knn classification (k = 128, distance = 2)
kknn_128_submission <- data.frame(id=test$id, Class_1=NA, Class_2=NA, Class_3=NA, Class_4=NA, Class_5=NA, Class_6=NA, Class_7=NA, Class_8=NA, Class_9=NA)
kknn.128 <- kknn(target ~ .-id, train, test, na.action = na.omit(), k = 128, distance = 2, kernel = "optimal")
kknn_128_submission[,2:10] <- kknn.128$prob
write.csv(kknn_128_submission, file="kknn_128_submission.csv", row.names = FALSE)

```




```{r}
# Code for cross-validation
# TAKES MAJOR TIME ... IF I GET RICH AND BUY A WORKSTATION ...
# Creating folds with caret
folds <- createFolds(train$target, k = 10, list = FALSE)
train$fold <- folds

# Create vector to store log-loss for each fold and execution time
fold_log_loss <- c()
fold_rf_exec_time <- c()

# K-fold cross-validation
for(k in 1:10){
  
  # Measure execution time
  start_time <- Sys.time()
  
  # Seperate the training, testing folds    
  test_i <- which(train$fold == k)
  train_fold <- train[-test_i, ]
  test_fold <- train[test_i, ]
  ans_fold <- answer[test_i,]
  train_fold$fold <- NULL
  test_fold$fold <- NULL
  
  ### THIS IS THE PART WHERE YOU TRY DIFFERENT MODELS
  
  # Perform simple random forest on trainfold
  prediction <- data.frame(id=test_fold$id, Class_1=NA, Class_2=NA, Class_3=NA, Class_4=NA, Class_5=NA, Class_6=NA, Class_7=NA, Class_8=NA, Class_9=NA)
  rf_train <- randomForest(train_fold[,c(-1,-95)], as.factor(train_fold$target), ntree=200, importance=TRUE)
  
  # Test simple random forest on testfold
  prediction[,2:10] <- predict(rf_train, test_fold[,-1], type="prob")
  
  # Evaluate log-loss
  fold_log_loss[k] <- logloss(as.matrix(prediction[,-1]),as.matrix(ans_fold[,-1]))
  
  end_time <- Sys.time()
  fold_rf_exec_time[k] <- end_time - start_time
}


fold_log_loss
mean(fold_log_loss)

```



