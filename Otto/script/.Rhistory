save(tmp, file = paste(paste("rf_cali_", toString(rounds), sep = ""), ".rData", sep = ""))
}
load("C:/Users/Willy/OneDrive/公用/台大/Senior courses/Second semester/Statistical Learning/Otto/script/rf_cali_1.rData")
tmp
rm(list=ls())
library(ggplot2)
library(randomForest)
library(readr)
library(dplyr)
library(caret)
library(adabag)
library(rminer)
library(class)
library(kknn)
library(xgboost)
library(methods)
library(data.table)
library(magrittr)
#options( java.parameters = "-Xmx2g" )
#library(extraTrees)
library(CORElearn)
source("model_ada.R")
source("model_xgboost.R")
source("rf_prob_calibration.R")
#source("rf_on_calibrate.R")
source("utility.R")
train <- read.csv("C:/Users/Willy/OneDrive/公用/台大/Senior courses/Second semester/Statistical Learning/Otto/data/train.csv", na.strings = "")
test <- read.csv("C:/Users/Willy/OneDrive/公用/台大/Senior courses/Second semester/Statistical Learning/Otto/data/test.csv", na.strings = "")
rm(list=ls())
library(ggplot2)
library(randomForest)
library(readr)
library(dplyr)
library(caret)
library(adabag)
library(rminer)
library(class)
library(kknn)
library(xgboost)
library(methods)
library(data.table)
library(magrittr)
#options( java.parameters = "-Xmx2g" )
#library(extraTrees)
library(CORElearn)
source("model_ada.R")
source("model_xgboost.R")
source("rf_prob_calibration.R")
#source("rf_on_calibrate.R")
source("utility.R")
train <- read.csv("C:/Users/Willy/OneDrive/公用/台大/Senior courses/Second semester/Statistical Learning/Otto/data/train.csv", na.strings = "")
test <- read.csv("C:/Users/Willy/OneDrive/公用/台大/Senior courses/Second semester/Statistical Learning/Otto/data/test.csv", na.strings = "")
# Get answer for train
train_answer <- myanswer(train)
#head(train_answer)
# Insufficient memory QAQQQ
rf_start_time <- Sys.time()
# Bagging random forest
for(rounds in 1 : 10){
rf_cali_bag_list <- rf_cali_cv_bagged(train, test, 1, 3)
save(rf_cali_bag_list, file = paste(paste("rf_cali_", toString(rounds), sep = ""), ".rData", sep = ""))
rm(rf_cali_bag_list)
}
rf_end_time <- Sys.time()
rf_exec_time <- rf_end_time - rf_start_time
print(rf_exec_time)
# Bagging xgb
#xgb_bag_list <- xgb_bagged(train, test, 10, 3)
#xgb_bagged_submission <- data.frame(id=test$id, Class_1=NA, Class_2=NA, Class_3=NA, Class_4=NA, Class_5=NA, Class_6=NA, Class_7=NA, Class_8=NA, Class_9=NA)
#xgb_bagged_submission[,2:10] <- xgb_bag_list[['test_meta_bagged']]
#write.csv(xgb_bagged_submission, file="xgb_bagged_submission.csv", row.names = FALSE)
for(rounds in 1 : 10){
load(paste(paste("rf_cali_", toString(rounds), sep = ""), ".rData", sep = ""))
}
paste(paste("rf_cali_", toString(rounds), sep = ""), ".rData", sep = "")
load("rf_cali_1.rData")
load("C:/Users/Willy/OneDrive/公用/台大/Senior courses/Second semester/Statistical Learning/Otto/script/rf_cali_1.rData")
paste("rf_cali_bag_list", toString(rounds), sep = "")
for(rounds in 1 : 10){
load(paste(paste("rf_cali_", toString(rounds), sep = ""), ".rData", sep = ""))
paste("rf_cali_bag_list", toString(rounds), sep = "") <- rf_cali_bag_list
}
rf_cali_bag_list <- list()
for(rounds in 1 : 10){
load(paste(paste("rf_cali_", toString(rounds), sep = ""), ".rData", sep = ""))
rf_bag_list[[rounds]] <- rf_cali_bag_list
}
rf_bag_list <- list()
for(rounds in 1 : 10){
load(paste(paste("rf_cali_", toString(rounds), sep = ""), ".rData", sep = ""))
rf_bag_list[[rounds]] <- rf_cali_bag_list
}
rf_bag_list
rf_bag_list[[1]]
rf_bag_list[[1]][[1]]
str(rf_bag_list[[1]])
str(rf_bag_list[[1]][[1]])
rf_bag_list[[1]][[1]]
rf_bag_list[[1]][[1]] + rf_bag_list[[2]][[1]]
for(rounds in 1 : 10){
rf_train_meta <- ifelse(rounds == 1, rf_bag_list[[1]][[1]], rf_train_meta + rf_bag_list[[rounds]][[1]])
rf_test_meta <- ifelse(rounds == 1, rf_bag_list[[1]][[2]], rf_test_meta + rf_bag_list[[rounds]][[2]])
}
rf_train_meta
rf_train_meta = rf_train_meta / 10
rf_train_meta
str(rf_train_meta)
str(rf_bag_list[[1]][[1]])
str(rf_bag_list[[2]][[1]])
rf_train_meta
rf_bag_list[[1]][[1]] + rf_bag_list[[2]][[1]] + rf_bag_list[[3]][[1]] + rf_bag_list[[4]][[1]]
for(rounds in 1 : 10){
if(rounds == 1){
print("hi")
rf_train_meta <- rf_bag_list[[1]][[1]]
rf_test_meta <- rf_bag_list[[1]][[2]]
}
else{
rf_train_meta <- rf_train_meta + rf_bag_list[[rounds]][[1]]
rf_test_meta <- rf_test_meta + rf_bag_list[[rounds]][[2]]
}
rf_train_meta
str(rf_train_meta)
rf_train_meta[,1:9] <- rf_train_meta[,1:9] / 10
rf_train_meta
rf_test_meta
rf_test_meta <- rf_test_meta / 10
rf_test_meta
rf_calibrated_submission_bagged_10 <- data.frame(id=test$id, Class_1=NA, Class_2=NA, Class_3=NA, Class_4=NA, Class_5=NA, Class_6=NA, Class_7=NA, Class_8=NA, Class_9=NA)
rf_calibrated_submission_bagged_10[,2:10] <- rf_test_meta
rf_calibrated_submission_bagged_10
write.csv(rf_calibrated_submission_bagged_10, file="rf_calibrated_submission_bagged_10.csv", row.names = FALSE)
rm(rf_cali_bag_list)
rm(rf_calibrated_submission_bagged_10)
rf_train_meta
rf_test_meta
rf_test_meta <- as.data.frame(rf_test_meta)
rf_bag_list <- list()
for(rounds in 1 : 10){
load(paste(paste("rf_cali_", toString(rounds), sep = ""), ".rData", sep = ""))
rf_bag_list[[rounds]] <- rf_cali_bag_list
if(rounds == 1){
train_meta_rf <- rf_bag_list[[1]][[1]]
test_meta_rf <- rf_bag_list[[1]][[2]]
}
else{
train_meta_rf <- train_meta_rf + rf_bag_list[[rounds]][[1]]
test_meta_rf <- test_meta_rf + rf_bag_list[[rounds]][[2]]
}
train_meta_rf[,1:9] <- train_meta_rf[,1:9] / 10
test_meta_rf <- test_meta_rf / 10
test_meta_rf <- as.data.frame(test_meta_rf)
train_meta_rf
rownames(rf_test_meta) <-
test_meta_rf
test_meta_rf <- test_meta_rf * 10
test_meta_rf <- as.data.frame(test_meta_rf)
test_meta_rf
sum(test_meta_rf[1,])
rf_bag_list <- list()
for(rounds in 1 : 10){
load(paste(paste("rf_cali_", toString(rounds), sep = ""), ".rData", sep = ""))
rf_bag_list[[rounds]] <- rf_cali_bag_list
if(rounds == 1){
train_meta_rf <- rf_bag_list[[1]][[1]]
test_meta_rf <- rf_bag_list[[1]][[2]]
}
else{
train_meta_rf <- train_meta_rf + rf_bag_list[[rounds]][[1]]
test_meta_rf <- test_meta_rf + rf_bag_list[[rounds]][[2]]
}
train_meta_rf[,1:9] <- train_meta_rf[,1:9] / 10
test_meta_rf
test_meta_rf <- test_meta_rf / 10
sum(test_meta_rf[1,])
sum(train_meta_rf[1,])
load("C:/Users/Willy/Desktop/xgb.RData")
xgb_bagged_submission
sum(xgb_bagged_submission[1,])
xgb_bagged_submission
sum(xgb_bagged_submission[1,2:10])
load("C:/Users/Willy/Desktop/metatrain.RData")
metatrain
metatrain[,1]
metatrain[,2]
metatrain
sum(metatrain[1,96:104])
xgb_bag_list
xgb_bag_list[[1]]
sum(xgb_bag_list[[1]][1,])
xgb_bag_list
sum(xgb_bag_list[[1]][1,])
sum(xgb_bag_list[[2]][1,])
sum(xgb_bag_list[[1]][1,])
xgb_bag_list[[1]]
metatrain
train_meta_xgb
xgbcolname <- c("xgb1", "xgb2", "xgb3", "xgb4", "xgb5", "xgb6", "xgb7", "xgb8", "xgb9")
train_meta_xgb <- as.data.frame(xgb_bag_list[['train_meta_bagged']])
test_meta_xgb <- as.data.frame(xgb_bag_list[['test_meta_bagged']])
colnames(train_meta_xgb) <- xgbcolname
colnames(test_meta_xgb) <- xgbcolname
rownames(train_meta_xgb) <- c(1:nrow(train_meta_xgb))
rownames(test_meta_xgb) <- c(1:nrow(test_meta_xgb))
train_meta_xgb
sum(train_meta_xgb[1:2:10])
sum(train_meta_xgb[1:])
sum(train_meta_xgb[1,])
train_meta_xgb
train_meta_xgb <- train_meta_xgb * 10
sum(train_meta_xgb[1,])
test_meta_xgb
sum(test_meta_xgb[1,])
sum(train_meta_xgb[1,])
sum(test_meta_xgb[1,])
train_meta_xgb
test_meta_xgb
train_meta_rf
sum(train_meta_rf[1,])
sum(train_meta_rf[2,])
sum(train_meta_rf[3,])
sum(train_meta_rf[4,])
for(i in 1 : nrow(train_meta_rf)){
train_meta_rf[i,] <- train_meta_rf[i,] / sum(train_meta_rf[i,])
}
train_meta_rf[1,] / sum(train_meta_rf[1,])
sum(train_meta_rf[1,] / sum(train_meta_rf[1,]))
for(i in 1 : nrow(train_meta_rf)){
train_meta_rf[i,] <- train_meta_rf[i,] / sum(train_meta_rf[i,])
}
test_meta_rf
train_meta_rf
as.matrix(train_meta_rf)
train_meta_rf <- as.matrix(train_meta_rf) / rowSums(as.matrix(train_meta_rf))
train_meta_rf
sum(train_meta_rf[1,])
sum(train_meta_rf[2,])
test_meta_rf
test_meta_rf <- test_meta_rf / rowSums(test_meta_rf)
test_meta_rf
train_meta_rf <- as.data.frame(train_meta_rf)
test_meta_rf <- as.data.frame(test_meta_rf)
rfcolname <- c("rf1", "rf2", "rf3", "rf4", "rf5", "rf6", "rf7", "rf8", "rf9")
colnames(train_meta_rf) <- rfcolname
colnames(test_meta_rf) <- rfcolname
rownames(train_meta_rf) <- c(1:nrow(train_meta_rf))
rownames(test_meta_rf) <- c(1:nrow(test_meta_rf))
train_meta_rf
test_meta_rf
train_meta_xgb
sum(train_meta_xgb[1,])
sum(test_meta_xgb[1,])
str(train)
metatrain <- cbind(train[,1:94], train_meta_xgb)
metatrain
metatrain <- cbind(metatrain, train_meta_rf)
metatrain
head(test)
metatrain <- cbind(train[,1:94], train_meta_xgb)
metatrain <- cbind(metatrain, train_meta_rf)
metatest <- cbind(test, test_meta_xgb)
metatest <- cbind(metatest, test_meta_rf)
metatrain
metatrain[1:96:104]
metatrain[1,96:104]
sum(metatrain[1,96:104])
train_meta_xgb
sum(train_meta_xgb[1])
sum(train_meta_xgb[1,])
metatrain[1,96:104]
sum(metatrain[1,95:103])
sum(metatrain[2,95:103])
sum(metatrain[1,104:112])
metatest
metatrain[1,95:103]
metatest[1,95:103]
metatrain[1,95:103]
metatest[1,95:103]
sum(metatest[1,95:103])
sum(metatrain[1,95:103])
sum(metatest[1,104:112])
sum(metatrain[1,95:103])
sum(metatest[1,95:103])
sum(metatrain[1,104:112])
sum(metatest[1,104:112])
train
target <- train$target
metatrain <- cbind(metatrain, target)
meta <- list(metatrain = metatrain, metatest = metatest)
save(meta, file="meta.RData")
rm(list=ls())
load("F:/otto data/rf.RData")
View(rf_calibrated_submission_bagged)
View(train_meta_bagged)
View(test_predict_bagged)
train_meta_list
test_meta_knn128 <- read.csv("kknn_128_submission.csv")
test_meta_knn_128 <- read.csv("kknn_128_submission.csv")
test_meta_knn_128
sum(test_meta_knn_128[1,])
sum(test_meta_knn_128[2,])
memory.limit(size=NA)
memory.limit()
rm(list=ls())
load("meta.RData")
test_meta_knn_128 <- read.csv("kknn_128_submission.csv")
train <- read.csv("C:/Users/Willy/OneDrive/公用/台大/Senior courses/Second semester/Statistical Learning/Otto/data/train.csv", na.strings = "")
test <- read.csv("C:/Users/Willy/OneDrive/公用/台大/Senior courses/Second semester/Statistical Learning/Otto/data/test.csv", na.strings = "")
library(ggplot2)
library(randomForest)
library(readr)
library(dplyr)
library(caret)
library(adabag)
library(rminer)
library(class)
library(kknn)
library(xgboost)
library(methods)
library(data.table)
library(magrittr)
#options( java.parameters = "-Xmx2g" )
#library(extraTrees)
library(CORElearn)
source("model_ada.R")
source("model_xgboost.R")
source("rf_prob_calibration.R")
#source("rf_on_calibrate.R")
source("utility.R")
# Get answer for train
train_answer <- myanswer(train)
#head(train_answer)
test_meta_knn_128 <- read.csv("kknn_128_submission.csv")
kknn_128_train <- kknn(target ~ .-id, train, train[,-95], na.action = na.omit(), k = 128, distance = 2, kernel = "optimal")
train_meta_knn_128 <- kknn_128_train$prob
train_meta_knn_128
sum(train_meta_knn_128[1,])
train_meta_knn_128
knncolname <- c("knn1", "knn2", "knn3", "knn4", "knn5", "knn6", "knn7", "knn8", "knn9")
train_meta_knn_128
train_meta_knn <- as.data.frame(train_meta_knn_128)
test_meta_knn_128
train_meta_knn <- as.data.frame(train_meta_knn_128)
test_meta_knn <- as.data.frame(test_meta_knn_128)
colnames(train_meta_knn) <- knncolname
colnames(test_meta_knn) <- knncolname
rownames(train_meta_knn) <- c(1:nrow(train_meta_knn))
rownames(test_meta_knn) <- c(1:nrow(test_meta_knn))
train_meta_knn
metatrain <- cbind(train[,1:94], train_meta_xgb)
metatrain <- cbind(metatrain, train_meta_knn)
load("meta.Rdata")
meta
metatrain <- meta[[1]]
metatest <- meta[[2]]
metatrain$target <- NULL
metatrain <- cbind(metatrain, train_meta_knn)
target <- train$target
metatrain <- cbind(metatrain, target)
metatest <- cbind(metatest, test_meta_knn)
meta <- list(metatrain = metatrain, metatest = metatest)
save(meta, file="meta.RData")
target <- train$target
metatrain <- cbind(metatrain, target)
metatrain
metatrain$target <- NULL
metatets
metatest
train_meta_knn
test_meta_knn
test_meta_knn_128
test_meta_knn <- as.data.frame(test_meta_knn_128[2:10])
test_meta_knn
train_meta_knn <- as.data.frame(train_meta_knn_128)
test_meta_knn <- as.data.frame(test_meta_knn_128[2:10])
colnames(train_meta_knn) <- knncolname
colnames(test_meta_knn) <- knncolname
rownames(train_meta_knn) <- c(1:nrow(train_meta_knn))
rownames(test_meta_knn) <- c(1:nrow(test_meta_knn))
metatrain
metatrain <- NULL
metatrain <- cbind(train[,1:94], train_meta_xgb)
metatrain <- meta[[1]]
metatrain
metatest
metatest[,114:123] <- NULL
metatest
metatest[,113] <- NULL
metatest <- cbind(metatest, test_meta_knn)
metatest
metatrain
metatrain[,113:122]
summetatrain[,113:122]
sum(metatrain[,113:122])
sum(metatrain[1,113:122])
metatrain[1,113:122]
metatrain[1,113:121]
sum(metatrain[1,113:121])
sum(metatest[1,113:121])
meta <- list(metatrain = metatrain, metatest = metatest)
save(meta, file="meta.RData")
metatrain
metatrain[,1]
metatrain
metatrain[,2:94] <- NULL
metatest
metatest[,2:94] <- NULL
metatrain
metalist <- toxgbmatrix(metatrain, metatest)
metalist
param <- list(booster = "gbtree", objective = "multi:softprob", eval_metric = "mlogloss",  eta = 0.1, gamma = 0, max_depth = 5, min_child_weight = 3, subsample=0.8, colsample_bytree=0.8, num_class = 9)
xgb_meta_cv <- xgb.cv( params = param, data = metalist[['train_matrix']], nrounds = 10000, nfold = 3, label = metalist[['num_target']], prediction = FALSE, showsd = T, stratified = T, early_stopping_rounds = 20, maximize = F, print_every_n = 5)
nrounds1 <- xgb_meta_cv$best_iteration
xgbgrid <- expand.grid(nrounds=nrounds1, eta = 0.1, gamma = 0, max_depth = c(3,5,7,9,12), min_child_weight = c(1,3,5), subsample=0.8, colsample_bytree=0.8)
source('C:/Users/Willy/OneDrive/公用/台大/Senior courses/Second semester/Statistical Learning/Otto/script/model_xgboost.R', echo=TRUE)
source("model_xgboost.R")
metatrain
metalist <- toxgbmatrix(metatrain, metatest)
metalist[['target']]
source('C:/Users/Willy/OneDrive/公用/台大/Senior courses/Second semester/Statistical Learning/Otto/script/model_xgboost.R', echo=TRUE)
source("model_xgboost.R")
metalist <- toxgbmatrix(metatrain, metatest)
metalist[['target']]
as.factor(metalist[['target']])
xgb_trcontrol_1 = trainControl(method = "cv", number = 3, verboseIter = TRUE, returnData = TRUE, returnResamp = "all", classProbs = TRUE, summaryFunction = mnLogLoss,
allowParallel = TRUE)
xgbgrid <- expand.grid(nrounds=nrounds1, eta = 0.1, gamma = 0, max_depth = c(3,5,7,9,12), min_child_weight = c(1,3,5), subsample=0.8, colsample_bytree=0.8)
tuning_top <- train(x = metalist[['train_matrix']], y = as.factor(metalist[['target']]), trControl = xgb_trcontrol_1, tuneGrid = xgbgrid, method = "xgbTree")
xgbgrid <- expand.grid(nrounds=nrounds1, eta = 0.1, gamma = 0, max_depth = c(4,5,6), min_child_weight = c(1,2,3), subsample=0.8, colsample_bytree=0.8)
xgb_trcontrol_1 = trainControl(method = "cv", number = 3, verboseIter = TRUE, returnData = TRUE, returnResamp = "all", classProbs = TRUE, summaryFunction = mnLogLoss,
allowParallel = TRUE)
tuning_top <- train(x = metalist[['train_matrix']], y = as.factor(metalist[['target']]), trControl = xgb_trcontrol_1, tuneGrid = xgbgrid, method = "xgbTree")
xgbgrid <- expand.grid(nrounds=nrounds1, eta = 0.1, gamma = 0, max_depth = 5, min_child_weight = c(0.7,0.8,0.9,1,1.1,1.2,1.3), subsample=0.8, colsample_bytree=0.8)
xgb_trcontrol_1 = trainControl(method = "cv", number = 3, verboseIter = TRUE, returnData = TRUE, returnResamp = "all", classProbs = TRUE, summaryFunction = mnLogLoss,
allowParallel = TRUE)
tuning_top <- train(x = metalist[['train_matrix']], y = as.factor(metalist[['target']]), trControl = xgb_trcontrol_1, tuneGrid = xgbgrid, method = "xgbTree")
xgbgrid <- expand.grid(nrounds=nrounds1, eta = 0.1, gamma = 0, max_depth = 5, min_child_weight = c(1.05,1.1,1.15), subsample=0.8, colsample_bytree=0.8)
xgb_trcontrol_1 = trainControl(method = "cv", number = 3, verboseIter = TRUE, returnData = TRUE, returnResamp = "all", classProbs = TRUE, summaryFunction = mnLogLoss,
allowParallel = TRUE)
save(tuning_top, file="tuning_top.RData")
tuning_top <- train(x = metalist[['train_matrix']], y = as.factor(metalist[['target']]), trControl = xgb_trcontrol_1, tuneGrid = xgbgrid, method = "xgbTree")
xgbgrid <- expand.grid(nrounds=nrounds1, eta = 0.1, gamma = 0, max_depth = 5, min_child_weight = c(1.15,1.17,1.19), subsample=0.8, colsample_bytree=0.8)
tuning_top <- train(x = metalist[['train_matrix']], y = as.factor(metalist[['target']]), trControl = xgb_trcontrol_1, tuneGrid = xgbgrid, method = "xgbTree")
xgb_trcontrol_1 = trainControl(method = "cv", number = 3, verboseIter = TRUE, returnData = TRUE, returnResamp = "all", classProbs = TRUE, summaryFunction = mnLogLoss,
allowParallel = TRUE)
xgbgrid <- expand.grid(nrounds=nrounds1, eta = 0.1, gamma = c(0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1), max_depth = 5, min_child_weight = 1.17, subsample=0.8, colsample_bytree=0.8)
tuning_top <- train(x = metalist[['train_matrix']], y = as.factor(metalist[['target']]), trControl = xgb_trcontrol_1, tuneGrid = xgbgrid, method = "xgbTree")
param <- list(booster = "gbtree", objective = "multi:softprob", eval_metric = "mlogloss",  eta = 0.1, gamma = 0.9, max_depth = 5, min_child_weight = 1.17, subsample=0.8, colsample_bytree=0.8, num_class = 9)
xgb_meta_cv2 <- xgb.cv( params = param, data = metalist[['train_matrix']], nrounds = 10000, nfold = 3, label = metalist[['num_target']], prediction = FALSE, showsd = T, stratified = T, early_stopping_rounds = 20, maximize = F, print_every_n = 5)
nrounds2 <- xgb_meta_cv2$best_iteration
nrounds2
nrounds1
xgbgrid <- expand.grid(nrounds=nrounds1, eta = 0.1, gamma = c(0.85,0.9,0.95), max_depth = 5, min_child_weight = 1.17, subsample=0.8, colsample_bytree=0.8)
xgb_trcontrol_1 = trainControl(method = "cv", number = 3, verboseIter = TRUE, returnData = TRUE, returnResamp = "all", classProbs = TRUE, summaryFunction = mnLogLoss,
allowParallel = TRUE)
tuning_top <- train(x = metalist[['train_matrix']], y = as.factor(metalist[['target']]), trControl = xgb_trcontrol_1, tuneGrid = xgbgrid, method = "xgbTree")
xgbgrid <- expand.grid(nrounds=nrounds1, eta = 0.1, gamma = c(0.83,0.85,0.87), max_depth = 5, min_child_weight = 1.17, subsample=0.8, colsample_bytree=0.8)
tuning_top <- train(x = metalist[['train_matrix']], y = as.factor(metalist[['target']]), trControl = xgb_trcontrol_1, tuneGrid = xgbgrid, method = "xgbTree")
xgbgrid <- expand.grid(nrounds=nrounds1, eta = 0.1, gamma = c(0.82,0.83,0.84), max_depth = 5, min_child_weight = 1.17, subsample=0.8, colsample_bytree=0.8)
param <- list(booster = "gbtree", objective = "multi:softprob", eval_metric = "mlogloss",  eta = 0.1, gamma = 0.83, max_depth = 5, min_child_weight = 1.17, subsample=0.8, colsample_bytree=0.8, num_class = 9)
xgb_meta_cv2 <- xgb.cv( params = param, data = metalist[['train_matrix']], nrounds = 10000, nfold = 3, label = metalist[['num_target']], prediction = FALSE, showsd = T, stratified = T, early_stopping_rounds = 20, maximize = F, print_every_n = 5)
nrounds2 <- xgb_meta_cv2$best_iteration
nrounds2
xgbgrid <- expand.grid(nrounds=nrounds2, eta = 0.1, gamma = 0.83, max_depth = 5, min_child_weight = 1.17, subsample=c(0.5,0.6,0.7,0.8,0.9), colsample_bytree=c(0.5,0.6,0.7,0.8,0.9))
tuning_top <- train(x = metalist[['train_matrix']], y = as.factor(metalist[['target']]), trControl = xgb_trcontrol_1, tuneGrid = xgbgrid, method = "xgbTree")
xgbgrid <- expand.grid(nrounds=nrounds2, eta = 0.1, gamma = 0.83, max_depth = 5, min_child_weight = 1.17, subsample=c(0.6,0.7,0.8,0.9), colsample_bytree=c(0.6,0.7,0.8,0.9))
tuning_top <- train(x = metalist[['train_matrix']], y = as.factor(metalist[['target']]), trControl = xgb_trcontrol_1, tuneGrid = xgbgrid, method = "xgbTree")
xgbgrid <- expand.grid(nrounds=nrounds2, eta = 0.1, gamma = 0.83, max_depth = 5, min_child_weight = 1.17, subsample=c(0.85, 0.9, 0.95), colsample_bytree=c(0.75,0.8,0.85))
tuning_top <- train(x = metalist[['train_matrix']], y = as.factor(metalist[['target']]), trControl = xgb_trcontrol_1, tuneGrid = xgbgrid, method = "xgbTree")
xgbgrid <- expand.grid(nrounds=nrounds2, eta = 0.1, gamma = 0.83, max_depth = 5, min_child_weight = 1.17, subsample=0.9, colsample_bytree=c(0.83,0.84,0.85,0.86,0.87))
tuning_top <- train(x = metalist[['train_matrix']], y = as.factor(metalist[['target']]), trControl = xgb_trcontrol_1, tuneGrid = xgbgrid, method = "xgbTree")
param <- list(booster = "gbtree", objective = "multi:softprob", eval_metric = "mlogloss",  eta = 0.1, gamma = 0.83, max_depth = 5, min_child_weight = 1.17, subsample=0.9, colsample_bytree=0.86, num_class = 9)
param <- list(booster = "gbtree", objective = "multi:softprob", eval_metric = "mlogloss",  eta = 0.01, gamma = 0.83, max_depth = 5, min_child_weight = 1.17, subsample=0.9, colsample_bytree=0.86, num_class = 9)
xgb_meta_cv2 <- xgb.cv( params = param, data = metalist[['train_matrix']], nrounds = 10000, nfold = 3, label = metalist[['num_target']], prediction = FALSE, showsd = T, stratified = T, early_stopping_rounds = 20, maximize = F, print_every_n = 5)
nrounds2 <- xgb_meta_cv2$best_iteration
nrounds2
param <- list(booster = "gbtree", objective = "multi:softprob", eval_metric = "mlogloss",  eta = 0.01, gamma = 0.83, max_depth = 5, min_child_weight = 1.17, subsample = 0.9, colsample_bytree = 0.86, num_class = 9)
xgboost_final <- data.frame(id=test$id, Class_1=NA, Class_2=NA, Class_3=NA, Class_4=NA, Class_5=NA, Class_6=NA, Class_7=NA, Class_8=NA, Class_9=NA)
rm(xgboost_final)
xgboost_final_submission <- data.frame(id=test$id, Class_1=NA, Class_2=NA, Class_3=NA, Class_4=NA, Class_5=NA, Class_6=NA, Class_7=NA, Class_8=NA, Class_9=NA)
param <- list(booster = "gbtree", objective = "multi:softprob", eval_metric = "mlogloss",  eta = 0.01, gamma = 0.83, max_depth = 5, min_child_weight = 1.17, subsample = 0.9, colsample_bytree = 0.86, num_class = 9)
xgb_final <- xgboost(param = param, data = metalist[['train_matrix']], label = metalist[['num_target']], nrounds = 1417)
xgboost_submission[,2:10] <- matrix(predict(xgb_final, metalist[['test_matrix']]), ncol = 9, byrow = T)
xgboost_final_submission <- data.frame(id=test$id, Class_1=NA, Class_2=NA, Class_3=NA, Class_4=NA, Class_5=NA, Class_6=NA, Class_7=NA, Class_8=NA, Class_9=NA)
xgboost_final_submission[,2:10] <- matrix(predict(xgb_final, metalist[['test_matrix']]), ncol = 9, byrow = T)
xgboost_final_submission
write.csv(xgboost_final_submission, file="xgboost_final_submission.csv", row.names = FALSE)
metalist[['train_matrix']]
metalist[['test_matrix']]
xgb_final
str(xgb_final)
xgb_final[['evaluation_log']]
xgb_final[['evaluation_log']]$train_mlogloss[1417:2]
xgb_final[['evaluation_log']]$train_mlogloss
xgb_final[['evaluation_log']]$train_mlogloss[1417]
metatrain
library(kknn)
data(iris)
m <- dim(iris)[1]
val <- sample(1:m, size = round(m/3), replace = FALSE,
prob = rep(1/m, m))
iris.learn <- iris[-val,]
iris.valid <- iris[val,]
iris.learn
iris.valid
iris.learn
iris.learn
iris.valid
iris.kknn <- kknn(Species~., iris.learn, iris.valid, distance = 1,
kernel = "triangular")
summary(iris.kknn)
iris.learn
