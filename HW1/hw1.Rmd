  ---
title: "Statistical Learning HW1"
output: html_notebook
---
```{r}
library(dplyr)
```


```{r}
df1_train <- read.csv("C:/Users/Willy/OneDrive/公用/台大/Senior courses/Second semester/Statistical Learning/R/Class/data/df1_train.csv")
df1_test1 <- read.csv("C:/Users/Willy/OneDrive/公用/台大/Senior courses/Second semester/Statistical Learning/R/Class/data/df1_test1.csv")
df1_test1y <- read.csv("C:/Users/Willy/OneDrive/公用/台大/Senior courses/Second semester/Statistical Learning/R/Class/data/df1_test1y.csv")
```


```{r}
### This function takes a train set and a test set, and returns a predition on the test set with parametered obtained from the train set ###
gpredict <- function(df_train, df_test){
  ### Check for unique cases ###
  # Assume that df_test is not missing
  miss <- FALSE
  # Specify the length of a and b
  a_len <- 1
  b_len <- ncol(df_train) - a_len
  # Check whether df_test is missing or null
  if(missing(df_test) || is.null(df_test)){
    pred <- NULL
    miss <- TRUE
  }
  # Return NULL if df_test has different number of columns(features)
  else if((ncol(df_test) != b_len)){
    return(NULL)
  }
  ### Calculate stats for df_train ###
  # Keep only the first column of df_train for xa
  xa <- df_train[,1]
  # Compute the mean for xa
  mean_a <- mean(xa)
  # Keep all columns EXCEPT the first one WITHOUT dropping dimensions
  xb <- df_train[,-1,drop = FALSE]
  # Calculate column means for xb
  mean_b <- colMeans(xb, na.rm = FALSE)
  # Construct the covarianc matrix
  cov_max <- cov(df_train)
  # Construct each part of the covariance matrix for cov_ab and cov_bb
  total_len <- ncol(df_train)
  cov_ab <- cov_max[1:a_len, (a_len+1):total_len]
  cov_bb <- cov_max[(a_len+1):total_len,(a_len+1):total_len]
  # Calculate "predict" if df_test exists
  if(!miss){
    pred <- apply(df_test, 1, function(df) mean_a + cov_ab %*% solve(cov_bb) 
              %*% as.vector(unlist((df-mean_b), use.names = FALSE)))
  }
  # Construct the required list
  ret <- list(mua=mean_a, mub=mean_b, s_ab=cov_ab, s_bb=cov_bb, predict=pred)
  return(ret)
}

```


```{r}
### TEST ###
# Case 1 : train set with only two columns
out1 <- gpredict(df1_train[1:200,1:2],df1_test1[,1,drop=FALSE])
print(out1$predict)

# Case 2 : test set not provided
out2 <- gpredict(df1_train[1:200,])
print(out2$predict)

# Case 3 : test set is NULL
out3 <- gpredict(df1_train[1:200,], NULL)
print(out3$predict)

# Case 4 : test set with different number of columns(features)
out4 <- gpredict(df1_train[1:200,],df1_test1[,-1])
print(out4$predict)
print(out4)

# Case 5 : normal case 
out5 <- gpredict(df1_train[1:200,],df1_test1)
print(out5$predict)
# Value check
mae1a <- mean(abs(df1_test1y[,1] - out5$predict))
print(mae1a)

```


```{r}
### This function takes mu(the original MLE estimator of mu), s(the original MLE estimator of sigma), n(the number of observations in the original dataset), and x(the new observation), and returns the new mu, sigma and n ###
mle_update <- function(mu_old, s_old, n_old, x_new){
  n_new <- n_old + 1
  mu_new <- mu_old + (1 / n_new) * (x_new-mu_old)
  s_new <- (n_old / n_new) * s_old + (n_old * (n_new ^ (-3))) * ((x_new - mu_old) %*% t(x_new - mu_old)) + 1 / n_new * ((x_new - mu_new) %*% t(x_new - mu_new))
  
  # Construct the required list
  ret <- list(mu=mu_new, s=s_new, n=n_new)
  return(ret)
}

```

```{r}
### Testing ###
set.seed(1223) 
nobs = 3 
nfeature = 7 
rawdata=matrix(runif(nobs*nfeature), nrow=nobs, ncol=nfeature)
data1 = rawdata[1:(nobs-1),]
#rawdata
xn = rawdata[nobs,] 
cov1 = cov(data1)*(nrow(data1)-1) / nrow(data1) 
mu1 = colMeans(data1)
out1 = mle_update(mu1, cov1, nrow(data1), xn)
print(out1$mu[1:3])
print(out1$s[1:3,1:3])
print(out1$n)
```

